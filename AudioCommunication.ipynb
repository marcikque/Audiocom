{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<p align=\"right\" width=\"100%\"><img width=\"200px\" height=\"auto\" src=\"../Admin/eth_logo_kurz_pos.png\">\n",
    "\n",
    "# Mobile Computing"
   ],
   "id": "835882cada4330ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise: Audio Communication\n",
    "### Prerequisites\n",
    "\n",
    "This Jupyter Notebook has been tested with Visual Studio Code, running in a local Python environment."
   ],
   "id": "f0ca342118bf402a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%pip install --quiet matplotlib scipy soundfile numpy\n",
    "from matplotlib import mlab\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from scipy.signal import get_window\n",
    "from scipy.fft import fft"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Sound Visualization\n",
    "#### 1.1 Loading and preprocessing the audio file\n",
    "The first step is to load the audio signal and normalize it."
   ],
   "id": "ba9e02e7ace9aca9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_audio(audio_file, time_interval):\n",
    "    try:\n",
    "        file_info = sf.info(audio_file)\n",
    "    except RuntimeError as e:\n",
    "        raise RuntimeError(f\"Error reading the audio file: {e}\") \n",
    "    \n",
    "    sample_rate = file_info.samplerate\n",
    "    \n",
    "    start_sample = int(round(time_interval[0] * sample_rate))\n",
    "    end_sample = int(round(time_interval[1] * sample_rate))\n",
    "    start_sample = max(start_sample, 0)\n",
    "    end_sample = min(end_sample, file_info.frames)\n",
    "    \n",
    "    try:\n",
    "        with sf.SoundFile(audio_file) as f:\n",
    "            f.seek(start_sample)\n",
    "            audio_data = f.read(end_sample - start_sample)\n",
    "    except RuntimeError as e:\n",
    "        raise RuntimeError(f\"Error reading samples from the audio file: {e}\")\n",
    "    \n",
    "    audio_data = np.array(audio_data)\n",
    "    \n",
    "    # Converting to mono if there are multiple channels\n",
    "    if audio_data.ndim > 1:\n",
    "        audio_data = audio_data[:, 0]\n",
    "    \n",
    "    # Normalizing the audio signal\n",
    "    audio_max = np.max(np.abs(audio_data))\n",
    "    if audio_max != 0:\n",
    "        audio_data = audio_data / audio_max\n",
    "    \n",
    "    audio_length = len(audio_data)\n",
    "    time_vector = np.arange(audio_length) / sample_rate\n",
    "    \n",
    "    return audio_data, sample_rate, time_vector, audio_length"
   ],
   "id": "36fe2dbb1c6f3bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With the `load_audio` function available, we can now select the audio file that we wish to visualize. \n",
    "\n",
    "Additionally, we also define the desired time interval for the visualization."
   ],
   "id": "4033635f557efff0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "audio_file = 'Sounds/MobileComp2.wav'\n",
    "time_interval = [1, 5]\n",
    "audio_data, sample_rate, time_vector, audio_length = load_audio(audio_file, time_interval)"
   ],
   "id": "9ee0becc975863aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.2 Time Domain Plot\n",
    "The time domain plot displays how a signal's amplitude changes over time."
   ],
   "id": "97118ea5a0c24b5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_time():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(time_vector, audio_data, linewidth=0.5)\n",
    "    plt.xlim([0, time_vector[-1]])\n",
    "    plt.ylim([-1.1*max(abs(audio_data)), 1.1*max(abs(audio_data))])\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.xlabel('Time [s]', fontsize=12)\n",
    "    plt.ylabel('Normalized Amplitude', fontsize=12)\n",
    "    plt.title(f'{audio_file} - Time Domain', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_time()"
   ],
   "id": "86cf5985f2139c6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.3 Frequency Domain Plot\n",
    "The frequency domain plot illustrates how a signal's energy is distributed across different frequency components. For this, we first translate the signal by using scipy's `fft` function."
   ],
   "id": "e482f19e976ef4d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_frequency():\n",
    "    window = get_window('hann', audio_length)\n",
    "    window_avg = np.sum(window) / audio_length\n",
    "    magnitude = np.abs(fft(audio_data * window)) # computing the FFT on the windowed audio signal\n",
    "    pos_freq = magnitude[:audio_length // 2] # extract first half (positive frequencies)\n",
    "    pos_freq = pos_freq / (audio_length / 2) # normalize\n",
    "    if len(pos_freq) > 0:\n",
    "        pos_freq[0] = pos_freq[0] / 2  # Correct the DC component\n",
    "    pos_freq = pos_freq / window_avg # compensate for windowing\n",
    "\n",
    "    freq_axis = np.linspace(0, sample_rate / 2, len(pos_freq)) # create frequency axis\n",
    "\n",
    "    pos_freq = np.maximum(pos_freq, 1e-10) # Prevent log of 0\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(freq_axis / 1000, 20 * np.log10(pos_freq), linewidth=0.5)\n",
    "    plt.xlim([freq_axis[0] / 1000, freq_axis[-1] / 1000])\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Frequency [kHz]', fontsize=12)\n",
    "    plt.ylabel('Amplitude [dBV]', fontsize=12)\n",
    "    plt.title(f'{audio_file} - Frequency Domain', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_frequency()"
   ],
   "id": "f7e224ad5dc140e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.4 Spectogram\n",
    "The spectogram displays how the frequency content of a signal changes over time.\n",
    "\n",
    "It effectively combines the time and frequency domain information."
   ],
   "id": "170b366a0ace8ebb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_spectogram():\n",
    "    FFT_win_size = 256\n",
    "    overlaps = int(FFT_win_size * 0.97)\n",
    "    window = get_window('hann', FFT_win_size)\n",
    "\n",
    "    spectral_power, freq, time_bins = mlab.specgram(\n",
    "        audio_data,\n",
    "        NFFT=FFT_win_size,\n",
    "        Fs=sample_rate,\n",
    "        window=window,\n",
    "        noverlap=overlaps,\n",
    "        scale_by_freq=False,\n",
    "        mode='psd'\n",
    "    )\n",
    "\n",
    "    spectral_power_dB = 10 * np.log10(spectral_power + 1e-10) # power to decibels\n",
    "\n",
    "    vmax = spectral_power_dB.max()\n",
    "    vmin = vmax - 100  # 100dB dynamic range\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.pcolormesh(time_bins, freq / 1000, spectral_power_dB, shading='gouraud', vmin=vmin, vmax=vmax, cmap='plasma')\n",
    "    plt.colorbar(label='Magnitude [dB]')\n",
    "    plt.xlabel('Time [s]', fontsize=12)\n",
    "    plt.ylabel('Frequency [kHz]', fontsize=12)\n",
    "    plt.title(f'{audio_file} - Spectrogram', fontsize=14)\n",
    "    plt.ylim(0, sample_rate / 2000)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_spectogram()"
   ],
   "id": "2a918d6c82bba656",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Audio Steganography\n",
    "#### 2.1. Prerequisites\n",
    "\n",
    "Audio Steganography is the practice of embedding information within audio files.\n",
    "\n",
    "While the error rate is often the most important metric for evaluating the success of steganography, it is equally important that the concealed data remains undetectable to unintended listeners.\n",
    "\n",
    "To start off, we first import and install the required packages.\n",
    "\n",
    "**NOTE**: Some of the subsequent sections may not function properly without FFMPEG."
   ],
   "id": "83dbfd8875290461"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install --quiet ipython numpy scipy matplotlib\n",
    "import os\n",
    "from AudioSteganography import *\n",
    "from IPython.display import Audio, display # the audio file display might not work in all IDEs\n",
    "\n",
    "if not os.path.exists('Out'):\n",
    "    os.makedirs('Out')"
   ],
   "id": "5eb5bc17c5a2cfc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Before we start, we define the audio file for embedding, specify the output file name, and set the message to embed.",
   "id": "b2e810095308a74b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_filename = \"Sounds/MobileComp2.wav\"\n",
    "output_filename = \"Out/stego_MobileComp2.wav\"\n",
    "message_filename = \"Messages/manimatter.txt\""
   ],
   "id": "ad607df5c83887e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(message_filename, 'r') as file:\n",
    "    message = file.read().replace('\\n', ' ')"
   ],
   "id": "39f0e31b1007ed24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.2. Encoding\n",
    "As with any file, we first verify if the audio file is even large enough to hold the text."
   ],
   "id": "fd0e9be0c3854425"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "message_length = len(message) * 8\n",
    "max_capacity = calculate_max_message_length(input_filename)\n",
    "print(f\"Maximum message length in bytes: {max_capacity}\")\n",
    "print(f\"Message size in bytes: {message_length}\")"
   ],
   "id": "bfd46405137da251",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If the message length is less than or equal to the maximum capacity, we proceed with embedding the message into the audio using phase coding.",
   "id": "ae5fb4f754f111d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embed_message(input_filename, output_filename, message)\n",
    "print(f'Embedded message:\\n{message}')\n",
    "\n",
    "display(Audio(filename=input_filename, autoplay=False))"
   ],
   "id": "e68e79fd9dcd0b69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.3. Decoding\n",
    "Now, we use the same algorithm in reverse to extract the message."
   ],
   "id": "146c31e92cafadea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "extracted_message = extract_message(output_filename, 8 * len(message))\n",
    "print(f'Extracted Message:\\n{extracted_message}')\n",
    "\n",
    "display(Audio(filename=output_filename, autoplay=False))"
   ],
   "id": "cfb1f9b4faca90da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.4. Accuracy\n",
    "Finally, we compare the original message to the extracted message to calculate the bit error rate.\n"
   ],
   "id": "ab0281514af7f1ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ber, _ = calculate_accuracy(message, extracted_message)\n",
    "print(f'Bit Error Rate (BER): {ber:.2%}')"
   ],
   "id": "6e35a3319601ded2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.5. Visualization\n",
    "Just like we did in section 1, we can also visualize the changes we introduced with our embedding."
   ],
   "id": "89ace67cc284b218"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "audio_file = 'Out/stego_MobileComp2.wav'\n",
    "time_interval = [1, 5]\n",
    "audio_data, sample_rate, time_vector, audio_length = load_audio(audio_file, time_interval)\n",
    "\n",
    "plot_spectogram()"
   ],
   "id": "796cddb24e96097a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Further Experiments\n",
    "Now that you have seen how to do it, it is your turn to use your own audio file.\n",
    "Below, you find a snippet that converts your audio file to the right format and right channel configuration for this notebook."
   ],
   "id": "92a8f975d2fb75b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_to_convert = \"AudioFile.xyz\"\n",
    "converted_file_name = \"AudioFile.wav\""
   ],
   "id": "208d2ec2ca910566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install --quiet pydub\n",
    "from pydub import AudioSegment"
   ],
   "id": "d9f35694896006e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sound = AudioSegment.from_file(file_to_convert)\n",
    "    \n",
    "if sound.channels > 1:\n",
    "    channels = sound.split_to_mono()\n",
    "    mono_sound = channels[0].overlay(channels[1])\n",
    "else:\n",
    "    mono_sound = sound\n",
    "\n",
    "mono_sound.export(converted_file_name, format=\"wav\")\n",
    "print(f\"File converted to mono WAV and saved as {converted_file_name}\")"
   ],
   "id": "8ed1dd021d46c3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\" width=\"100%\">═════════════════════════════════<br>\n",
    " Stefan Mangold (<a href=\"mailto:stefan.mangold@inf.ethz.ch\">stefan.mangold@inf.ethz.ch</a>)<br><img width=\"200px\" height=\"auto\" src=\"../Admin/eth_logo_kurz_pos.png\"></p>"
   ],
   "id": "6512cfc3658a4b3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
